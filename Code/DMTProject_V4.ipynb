{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisite packages installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOx1oaD-axQs",
    "outputId": "ecc9e30c-3ae1-4acc-db0a-772173d14bea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (3.8.5)\n",
      "Requirement already satisfied: requests in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (4.64.1)\n",
      "Requirement already satisfied: packaging in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from packaging->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: selenium in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (4.6.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: outcome in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n",
      "Requirement already satisfied: chromedriver-py in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (108.0.5359.22)\n",
      "Requirement already satisfied: PyPDF2 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (2.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from PyPDF2) (4.1.1)\n",
      "Requirement already satisfied: wordcloud in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: pillow in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/deveshsurve/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager\n",
    "!pip install selenium\n",
    "!pip install chromedriver-py\n",
    "!pip install PyPDF2\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaCnhER0JibP"
   },
   "source": [
    "## Configuring the json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SWvkzS6vJekG"
   },
   "outputs": [],
   "source": [
    "# Saved file for each job url\n",
    "JOBS_LINKS_JSON_FILE = r'/Users/deveshsurve/Downloads/indeed_jobs_links.json'\n",
    "# Saved file for each job info\n",
    "JOBS_INFO_JSON_FILE = r'/Users/deveshsurve/Downloads/indeed_jobs_infos.json'\n",
    "# Saved file for recommended jobs\n",
    "RECOMMENDED_JOBS_FILE = r'./data/recommended_jobs'\n",
    "# Path to webdriver exe\n",
    "WEBDRIVER_PATH = r'D:\\chromedriver\\chromedriver.exe'\n",
    "# City to search: Boston,MA\n",
    "JOB_LOCATIONS = ['Boston,MA']\n",
    "# Seach \"data scientist\" OR \"data+engineer\" OR \"data+analyst\" with quotation marks\n",
    "# JOB_SEARCH_WORDS = '\"data scientist\"+OR+\"data engineer\"+OR+\"data analyst\"'\n",
    "JOB_SEARCH_WORDS = 'data scientist'\n",
    "# To avoid same job posted multiple times, we only look back for 30 days\n",
    "DAY_RANGE = 305\n",
    "# Path to sample resume\n",
    "# SAMPLE_RESUME_PDF = r'C:/Users/TANNI/Desktop/CLASS NOTES/SEM4/CO_OP/RESUME/Tannishtha_Mandal_DS_ML.pdf'\n",
    "SAMPLE_RESUME_PDF = r'/Users/deveshsurve/Downloads/Devesh-Surve-Resume.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yQz1LcNJmNQ"
   },
   "source": [
    "## Web Scraping Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZmaugBTUHgjF"
   },
   "outputs": [],
   "source": [
    "import random, json\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time, os\n",
    "\n",
    "\n",
    "# We only collect max 500 \n",
    "max_results_per_city = 500\n",
    "# Number of jobs show on each result page\n",
    "page_record_limit = 50\n",
    "num_pages = int(max_results_per_city/page_record_limit)\n",
    "\n",
    "def get_jobs_info(search_location):\n",
    "    '''\n",
    "    Scrape from web or read from saved file\n",
    "    Input: \n",
    "        search_location - search job in a certain city. Input from command line.\n",
    "    Output: \n",
    "        jobs_info - a list that has info of each job i.e. link, location, title, company, salary, desc\n",
    "    '''\n",
    "    exists = os.path.isfile(JOBS_INFO_JSON_FILE)\n",
    "    # print(\"exists\", exists)\n",
    "    jobs_info = web_scrape(search_location)\n",
    "    if exists:\n",
    "        with open(JOBS_INFO_JSON_FILE, 'r') as fp:\n",
    "            jobs_info = json.load(fp)            \n",
    "    else:\n",
    "        jobs_info = web_scrape(search_location)\n",
    "        \n",
    "    return jobs_info\n",
    "        \n",
    "def web_scrape(search_location):\n",
    "    '''\n",
    "    Scrape jobs from indeed.ca\n",
    "    Input: \n",
    "        search_location - search job in a certain city. Input from commond line.\n",
    "    Output: \n",
    "        jobs_info - a list that has info of each job i.e. link, location, title, company, salary, desc\n",
    "    '''\n",
    "    # urls of all jobs\n",
    "    job_links = []\n",
    "    # Record time for web scraping\n",
    "    # start time\n",
    "    start = time.time() \n",
    "    # Launch webdriver\n",
    "    driver = webdriver.Chrome(WEBDRIVER_PATH)\n",
    "    job_locations = JOB_LOCATIONS\n",
    "    # If search location is defined, only search that location\n",
    "    if (len(search_location) > 0):\n",
    "        job_locations = [search_location]\n",
    "        \n",
    "    # Extract all job urls \n",
    "    for location in job_locations: \n",
    "        url = 'https://www.indeed.com/jobs?q='+ JOB_SEARCH_WORDS + '&l=' \\\n",
    "        + location + '&limit=' + str(page_record_limit) + '&fromage='+ str(DAY_RANGE)\n",
    "        # Set timeout\n",
    "        driver.set_page_load_timeout(80)\n",
    "        webdriver.DesiredCapabilities.CHROME[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  \n",
    "        for i in range(num_pages):            \n",
    "            try:\n",
    "                # Each job on the page's its url\n",
    "                from selenium.webdriver.common.by import By\n",
    "                job_names = []\n",
    "                company_names = []\n",
    "                job_locations =[]\n",
    "                job_descs = []\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//h2[@class='jobTitle css-1h4a4n5 eu4oa1w0']\"):\n",
    "                    job_name = job_each.text\n",
    "                    job_names.append(job_name)\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//a[@data-tn-element='companyName']\"):\n",
    "                    company_name = job_each.text\n",
    "                    company_names.append(company_name)\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//div[@class ='companyLocation']\"):\n",
    "                    job_location = job_each.text\n",
    "                    job_locations.append(job_location)\n",
    "                for job_each in driver.find_elements(By.XPATH, \"//div[@id = 'jobDescriptionText']\"):\n",
    "                    job_desc = job_each.text\n",
    "                    job_descs.append(job_desc)\n",
    "                cd = [job_each for job_each in driver.find_elements(By.XPATH, \"//div[@class = 'slider_container css-g7s71f eu4oa1w0']\")]\n",
    "                # [jd.text for jd in driver.find_elements(By.XPATH, \"//div[@id = 'jobDescriptionText']\")]\n",
    "                # cd[2].click()\n",
    "                \n",
    "                result = []\n",
    "                for i in range(5):\n",
    "                    driver.get(url)\n",
    "                    time.sleep(5)  \n",
    "                    cd = [job_each for job_each in driver.find_elements(By.XPATH, \"//div[@class = 'slider_container css-g7s71f eu4oa1w0']\")]\n",
    "                    if i<len(cd):\n",
    "                        cd[i].click()\n",
    "                        time.sleep(5) \n",
    "                        result.append([jd.text for jd in driver.find_elements(By.XPATH, \"//div[@id = 'jobDescriptionText']\")])\n",
    "                job_desc = [i[0] for i in result]\n",
    "                # print(\"\\n\\nresult:\",result)\n",
    "                # slider_container css-g7s71f eu4oa1w0\n",
    "                # print(\"job_names: \\n\", job_names)\n",
    "                # print(\"company_names: \\n\", company_names)\n",
    "                # print(\"location:\\n\", job_locations)\n",
    "                # print(\"job_Desc:\\n\", job_descs)\n",
    "                zipped = zip(job_names, company_names, job_locations, job_desc)\n",
    "                zipped_list = list(zipped)\n",
    "                # print(zipped_list)\n",
    "                print ('scraping {} page {}'.format(location, i+1))\n",
    "                # Go next page\n",
    "                driver.find_element(By.LINK_TEXT,'Next »').click()\n",
    "            except NoSuchElementException:\n",
    "                # If nothing find, we are at the end of all returned results\n",
    "                print (\"{} finished\".format(location))\n",
    "                break        \n",
    "            time.sleep(3)\n",
    "    with open(JOBS_INFO_JSON_FILE, 'w') as fp:\n",
    "        json.dump(zipped_list, fp)\n",
    "    # Close and quit webdriver\n",
    "    driver.quit()    \n",
    "    end = time.time() # end time\n",
    "    # Calculate web scaping time\n",
    "    scaping_time = (end-start)/60.\n",
    "    print('Took {0:.2f} minutes scraping {1:d} data scientist jobs'.format(scaping_time, len(zipped_list)))\n",
    "    return zipped_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8zfC01yJ39o"
   },
   "source": [
    "## Match Job Skills with Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pZOQZaqhKE1E"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from scipy.spatial.distance import cosine\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "# The following data science skill sets are modified from \n",
    "program_languages = ['bash','r','python','java','c++','ruby','perl','matlab','javascript','scala','php']\n",
    "analysis_software = ['excel','tableau','sas','spss','d3','saas','pandas','numpy','scipy','sps','spotfire','scikit','splunk','power','h2o']\n",
    "ml_framework = ['pytorch','tensorflow','caffe','caffe2','cntk','mxnet','paddle','keras','bigdl']\n",
    "bigdata_tool = ['hadoop','mapreduce','spark','pig','hive','shark','oozie','zookeeper','flume','mahout','etl']\n",
    "ml_platform = ['aws','azure','google','ibm']\n",
    "methodology = ['agile','devops','scrum']\n",
    "databases = ['sql','nosql','hbase','cassandra','mongodb','mysql','mssql','postgresql','oracle','rdbms','bigquery']\n",
    "overall_skills_dict = program_languages + analysis_software + ml_framework + bigdata_tool + databases + ml_platform + methodology\n",
    "education = ['master','phd','undergraduate','bachelor','mba']\n",
    "overall_dict = overall_skills_dict + education\n",
    "# specify the length of each minhash vector\n",
    "N = 32\n",
    "max_val = (2**8)-1\n",
    "# create N tuples that will serve as permutation functions\n",
    "# these permutation values are used to hash all input sets\n",
    "perms = [ (randint(0,max_val), randint(0,max_val)) for i in range(N)]\n",
    "# initialize a sample minhash vector of length N\n",
    "# each record will be represented by its own vec\n",
    "vec = [float('inf') for i in range(N)]\n",
    "\n",
    "\n",
    "class skill_keyword_match:\n",
    "    jobs_info_df = None\n",
    "    def __init__(self, jobs_list):\n",
    "        '''\n",
    "        Initialization - converts list to DataFrame\n",
    "        Input: \n",
    "            jobs_list (list): a list of all jobs info\n",
    "        Output: \n",
    "            None\n",
    "        '''\n",
    "        self.jobs_info_df = pd.DataFrame(jobs_list) \n",
    "#         jobs_info_df =pd.DataFrame(jobs_list)\n",
    "        #jobs_info_df.loc[len(jobs_info_df)] = jobs_list\n",
    "\n",
    "        self.jobs_info_df.rename(columns = {'0':'job_title','1':'company_name', '2':'location','3':'job_desc'})\n",
    "        print(\"Dataset:\\n\")\n",
    "        print(self.jobs_info_df.head(5))\n",
    "        \n",
    "        \n",
    "    def keywords_extract(self, text): \n",
    "        '''\n",
    "        Tokenize webpage text and extract keywords\n",
    "        Input: \n",
    "            text (str): text to extract keywords from\n",
    "        Output: \n",
    "            keywords (list): keywords extracted and filtered by pre-defined dictionary\n",
    "        '''        \n",
    "        text = re.sub(\"[^a-zA-Z+3]\",\" \", text) \n",
    "        text = text.lower().split()\n",
    "        stops = set(stopwords.words(\"english\")) \n",
    "        #filter out stop words in english language\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = list(set(text))\n",
    "        #keywords from the pre-defined skill dictionary\n",
    "        keywords = [str(word) for word in text if word in overall_dict]\n",
    "        return keywords\n",
    " \n",
    "    def keywords_count(self, keywords, counter): \n",
    "        '''\n",
    "        Count frequency of keywords\n",
    "        Input: \n",
    "            keywords (list): list of keywords\n",
    "            counter (Counter)\n",
    "        Output: \n",
    "            keyword_count (DataFrame index:keyword value:count)\n",
    "        '''           \n",
    "        keyword_count = pd.DataFrame(columns = ['Freq'])\n",
    "        for each_word in keywords: \n",
    "            keyword_count.loc[each_word] = {'Freq':counter[each_word]}\n",
    "        return keyword_count\n",
    "    \n",
    "    def exploratory_data_analysis(self):\n",
    "        '''\n",
    "        Exploratory data analysis\n",
    "        Input: \n",
    "            None\n",
    "        Output: \n",
    "            None\n",
    "        '''         \n",
    "        # Create a counter of keywords\n",
    "        doc_freq = Counter() \n",
    "        f = [doc_freq.update(item) for item in self.jobs_info_df['keywords']]\n",
    "        \n",
    "        #pre-defined skillset vocabulary in Counter\n",
    "        overall_skills_df = self.keywords_count(overall_skills_dict, doc_freq)\n",
    "        # Calculate percentage of required skills in all jobs\n",
    "        overall_skills_df['Freq_perc'] = (overall_skills_df['Freq'])*100/self.jobs_info_df.shape[0]\n",
    "        overall_skills_df = overall_skills_df.sort_values(by='Freq_perc', ascending=False)  \n",
    "        # Make bar plot \n",
    "        plt.figure(figsize=(14,8))\n",
    "        overall_skills_df.iloc[0:30, overall_skills_df.columns.get_loc('Freq_perc')].plot.bar()\n",
    "        plt.title('Percentage of Required Data Skills in Data Scientist Job Posts')\n",
    "        plt.ylabel('Percentage Required in Jobs (%)')\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.show()\n",
    "        \n",
    "         \n",
    "        # Let's look up education requirements\n",
    "        education_df = self.keywords_count(education, doc_freq)\n",
    "        # Merge undergrad with bachelor\n",
    "        education_df.loc['bachelor','Freq'] = education_df.loc['bachelor','Freq'] + education_df.loc['undergraduate','Freq'] \n",
    "        education_df.drop(labels='undergraduate', axis=0, inplace=True)\n",
    "        # Calculate percentage of required skills in all jobs\n",
    "        education_df['Freq_perc'] = (education_df['Freq'])*100/self.jobs_info_df.shape[0] \n",
    "        education_df = education_df.sort_values(by='Freq_perc', ascending=False)  \n",
    "        \n",
    "        \n",
    "    def get_jaccard_sim(self, x_set, y_set): \n",
    "        '''\n",
    "        Jaccard similarity or intersection over union measures similarity \n",
    "        between finite sample sets,  and is defined as size of intersection \n",
    "        divided by size of union of two sets. \n",
    "        \n",
    "        Input: \n",
    "            x_set (set)\n",
    "            y_set (set)\n",
    "        Output: \n",
    "            Jaccard similarity score\n",
    "        '''         \n",
    "        intersection = x_set.intersection(y_set)\n",
    "        return float(len(intersection)) / (len(x_set) + len(y_set) - len(intersection))\n",
    "    \n",
    "    \n",
    "    def cal_similarity(self, resume_keywords, location=None):\n",
    "        '''\n",
    "        Calculate similarity between keywords from resume and job posts\n",
    "        Input: \n",
    "            resume_keywords (list): resume keywords\n",
    "            location (str): city to search jobs\n",
    "        Output: \n",
    "            top_match (DataFrame): top job matches\n",
    "        '''     \n",
    "        num_jobs_return = 20\n",
    "        similarity = []\n",
    "#         j_info = self.jobs_info_df.loc[self.jobs_info_df[2]==location].copy() if len(location)>0 else self.jobs_info_df.copy()\n",
    "        j_info = self.jobs_info_df\n",
    "        if j_info.shape[0] < num_jobs_return:        \n",
    "            num_jobs_return = j_info.shape[0]  \n",
    "        for job_skills in j_info['keywords']:\n",
    "            similarity.append(self.get_jaccard_sim(set(resume_keywords), set(job_skills)))\n",
    "        j_info['similarity'] = similarity\n",
    "        top_match = j_info.sort_values(by='similarity', ascending=False).head(num_jobs_return)        \n",
    "        # Return top matched jobs\n",
    "        return top_match\n",
    "      \n",
    "        \n",
    "    def extract_jobs_keywords(self):\n",
    "        '''\n",
    "        Extract skill keywords from job descriptions and add a new column \n",
    "        Input: \n",
    "            None\n",
    "        Output: \n",
    "            None\n",
    "        \n",
    "        '''\n",
    "        self.jobs_info_df.head(5)\n",
    "        self.jobs_info_df['keywords'] = [self.keywords_extract(job_desc) for job_desc in self.jobs_info_df[3]]\n",
    "        \n",
    "        \n",
    "    def extract_resume_keywords(self, resume_pdf): \n",
    "        '''\n",
    "        Extract key skills from a resume \n",
    "        Input: \n",
    "            resume_pdf (str): path to resume PDF file\n",
    "        Output: \n",
    "            resume_skills (DataFrame index:keyword value:count): keywords counts\n",
    "        ''' \n",
    "        # Open resume PDF\n",
    "        resume_file = open(resume_pdf, 'rb')\n",
    "        # creating a pdf reader object\n",
    "        resume_reader = PyPDF2.PdfFileReader(resume_file)\n",
    "        # Read in each page in PDF\n",
    "        resume_content = [resume_reader.getPage(x).extractText() for x in range(resume_reader.numPages)]\n",
    "        # Extract key skills from each page\n",
    "        resume_keywords = [self.keywords_extract(page) for page in resume_content]\n",
    "        # Count keywords\n",
    "        resume_freq = Counter() \n",
    "        f = [resume_freq.update(item) for item in resume_keywords] \n",
    "        # Get resume skill keywords counts\n",
    "        resume_skills = self.keywords_count(overall_skills_dict, resume_freq)\n",
    "        \n",
    "        return(resume_skills[resume_skills['Freq']>0])\n",
    "            \n",
    "    def calculate_minhash(self, resume_keywords):\n",
    "        num_jobs_return = 10\n",
    "        j_info = self.jobs_info_df\n",
    "        minhash_similarity =[]\n",
    "        \n",
    "        for job_skills in j_info['keywords']:\n",
    "            #similarity.append(self.get_jaccard_sim(set(resume_keywords), set(job_skills)))\n",
    "\n",
    "        # specify some input sets\n",
    "            data_resume = set(resume_keywords)\n",
    "            data_job_keywords = set(job_skills)\n",
    "            # get the minhash vectors for each input set\n",
    "            vec1 = self.minhash(data_resume)\n",
    "            vec2 = self.minhash(data_job_keywords)\n",
    "\n",
    "            # divide both vectors by their max values to scale values {0:1}\n",
    "            vec1 = np.array(vec1) / max(vec1)\n",
    "            vec2 = np.array(vec2) / max(vec2)\n",
    "            cos_sim = 1 - cosine(vec1, vec2)\n",
    "            minhash_similarity.append(cos_sim)\n",
    "            # measure the similarity between the vectors using cosine similarity\n",
    "            print( '\\n Minhash using similarity:', cos_sim )\n",
    "            \n",
    "        j_info['Minhash_Similarity'] = minhash_similarity\n",
    "        top_match_minhash = j_info.sort_values(by='similarity', ascending=False).head(num_jobs_return)        \n",
    "        # Return top matched jobs\n",
    "        return top_match_minhash\n",
    "    \n",
    "    def minhash(self, s ,prime=4294967311):\n",
    "        vec = [float('inf') for i in range(N)]\n",
    "        \n",
    "        for val in s:\n",
    "            if not isinstance(val, int): val = hash(val)\n",
    "            for perm_idx, perm_vals in enumerate(perms):\n",
    "                a,b = perm_vals\n",
    "                output = (a * val + b) % prime\n",
    "                if vec[perm_idx]>output:\n",
    "                    vec[perm_idx] = output\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/2m60vf_d5z3b2hf93l6yznfr0000gn/T/ipykernel_2003/1331535218.py:47: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(WEBDRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping Boston,MA page 5\n",
      "Boston,MA finished\n",
      "Took 1.61 minutes scraping 5 data scientist jobs\n",
      "Dataset:\n",
      "\n",
      "                         0                         1  \\\n",
      "0  Analyst I, Data Science  Liberty Mutual Insurance   \n",
      "1       Data Scientist III              Novo Nordisk   \n",
      "2  Scientist, Data Science                    MAPFRE   \n",
      "3           Data Scientist             XPO Logistics   \n",
      "4  Analyst, Data Scientist            Analog Devices   \n",
      "\n",
      "                                            2  \\\n",
      "0                            Boston, MA 02101   \n",
      "1                               Lexington, MA   \n",
      "2                                  Boston, MA   \n",
      "3  Boston, MA 02111 \\n(Leather District area)   \n",
      "4          Boston, MA 02116 \\n(Back Bay area)   \n",
      "\n",
      "                                                   3  \n",
      "0  The Product Design and Modeling Department of ...  \n",
      "1  This is a high level Data Science role on our ...  \n",
      "2  The Product Design and Modeling Department of ...  \n",
      "3  Solutions driven success.\\n\\nXPO is a top ten ...  \n",
      "4  Keystone is a premier strategy and economics c...  \n",
      "resume skills :              Freq\n",
      "r              1\n",
      "python         1\n",
      "sas            1\n",
      "pandas         1\n",
      "numpy          1\n",
      "tensorflow     1\n",
      "keras          1\n",
      "hadoop         1\n",
      "spark          1\n",
      "mysql          1\n",
      "postgresql     1\n",
      "agile          1\n",
      "scrum          1\n",
      "Top matches with jaccard similarity\n",
      ":\n",
      "                         0                         1  \\\n",
      "3           Data Scientist             XPO Logistics   \n",
      "1       Data Scientist III              Novo Nordisk   \n",
      "0  Analyst I, Data Science  Liberty Mutual Insurance   \n",
      "2  Scientist, Data Science                    MAPFRE   \n",
      "4  Analyst, Data Scientist            Analog Devices   \n",
      "\n",
      "                                            2  \\\n",
      "3  Boston, MA 02111 \\n(Leather District area)   \n",
      "1                               Lexington, MA   \n",
      "0                            Boston, MA 02101   \n",
      "2                                  Boston, MA   \n",
      "4          Boston, MA 02116 \\n(Back Bay area)   \n",
      "\n",
      "                                                   3  \\\n",
      "3  Solutions driven success.\\n\\nXPO is a top ten ...   \n",
      "1  This is a high level Data Science role on our ...   \n",
      "0  The Product Design and Modeling Department of ...   \n",
      "2  The Product Design and Modeling Department of ...   \n",
      "4  Keystone is a premier strategy and economics c...   \n",
      "\n",
      "                                            keywords  similarity  \n",
      "3  [java, python, bachelor, master, sql, spark, s...    0.222222  \n",
      "1             [python, bachelor, master, sql, spark]    0.125000  \n",
      "0                                 [bachelor, master]    0.000000  \n",
      "2                                 [bachelor, master]    0.000000  \n",
      "4                  [undergraduate, master, mba, phd]    0.000000  \n",
      "\n",
      " Minhash using similarity: 0.3596507500826096\n",
      "\n",
      " Minhash using similarity: 0.344456048709576\n",
      "\n",
      " Minhash using similarity: 0.3596507500826096\n",
      "\n",
      " Minhash using similarity: 0.3934939617688564\n",
      "\n",
      " Minhash using similarity: 0.4356407929192594\n",
      "\n",
      "Top matches with cosine similarity using minhash\n",
      ":\n",
      "                         0                         1  \\\n",
      "3           Data Scientist             XPO Logistics   \n",
      "1       Data Scientist III              Novo Nordisk   \n",
      "0  Analyst I, Data Science  Liberty Mutual Insurance   \n",
      "2  Scientist, Data Science                    MAPFRE   \n",
      "4  Analyst, Data Scientist            Analog Devices   \n",
      "\n",
      "                                            2  \\\n",
      "3  Boston, MA 02111 \\n(Leather District area)   \n",
      "1                               Lexington, MA   \n",
      "0                            Boston, MA 02101   \n",
      "2                                  Boston, MA   \n",
      "4          Boston, MA 02116 \\n(Back Bay area)   \n",
      "\n",
      "                                                   3  \\\n",
      "3  Solutions driven success.\\n\\nXPO is a top ten ...   \n",
      "1  This is a high level Data Science role on our ...   \n",
      "0  The Product Design and Modeling Department of ...   \n",
      "2  The Product Design and Modeling Department of ...   \n",
      "4  Keystone is a premier strategy and economics c...   \n",
      "\n",
      "                                            keywords  similarity  \\\n",
      "3  [java, python, bachelor, master, sql, spark, s...    0.222222   \n",
      "1             [python, bachelor, master, sql, spark]    0.125000   \n",
      "0                                 [bachelor, master]    0.000000   \n",
      "2                                 [bachelor, master]    0.000000   \n",
      "4                  [undergraduate, master, mba, phd]    0.000000   \n",
      "\n",
      "   Minhash_Similarity  \n",
      "3            0.393494  \n",
      "1            0.344456  \n",
      "0            0.359651  \n",
      "2            0.359651  \n",
      "4            0.435641  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>keywords</th>\n",
       "      <th>similarity</th>\n",
       "      <th>Minhash_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>XPO Logistics</td>\n",
       "      <td>Boston, MA 02111 \\n(Leather District area)</td>\n",
       "      <td>Solutions driven success.\\n\\nXPO is a top ten ...</td>\n",
       "      <td>[java, python, bachelor, master, sql, spark, s...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.393494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist III</td>\n",
       "      <td>Novo Nordisk</td>\n",
       "      <td>Lexington, MA</td>\n",
       "      <td>This is a high level Data Science role on our ...</td>\n",
       "      <td>[python, bachelor, master, sql, spark]</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.344456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst I, Data Science</td>\n",
       "      <td>Liberty Mutual Insurance</td>\n",
       "      <td>Boston, MA 02101</td>\n",
       "      <td>The Product Design and Modeling Department of ...</td>\n",
       "      <td>[bachelor, master]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientist, Data Science</td>\n",
       "      <td>MAPFRE</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>The Product Design and Modeling Department of ...</td>\n",
       "      <td>[bachelor, master]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyst, Data Scientist</td>\n",
       "      <td>Analog Devices</td>\n",
       "      <td>Boston, MA 02116 \\n(Back Bay area)</td>\n",
       "      <td>Keystone is a premier strategy and economics c...</td>\n",
       "      <td>[undergraduate, master, mba, phd]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                         1  \\\n",
       "3           Data Scientist             XPO Logistics   \n",
       "1       Data Scientist III              Novo Nordisk   \n",
       "0  Analyst I, Data Science  Liberty Mutual Insurance   \n",
       "2  Scientist, Data Science                    MAPFRE   \n",
       "4  Analyst, Data Scientist            Analog Devices   \n",
       "\n",
       "                                            2  \\\n",
       "3  Boston, MA 02111 \\n(Leather District area)   \n",
       "1                               Lexington, MA   \n",
       "0                            Boston, MA 02101   \n",
       "2                                  Boston, MA   \n",
       "4          Boston, MA 02116 \\n(Back Bay area)   \n",
       "\n",
       "                                                   3  \\\n",
       "3  Solutions driven success.\\n\\nXPO is a top ten ...   \n",
       "1  This is a high level Data Science role on our ...   \n",
       "0  The Product Design and Modeling Department of ...   \n",
       "2  The Product Design and Modeling Department of ...   \n",
       "4  Keystone is a premier strategy and economics c...   \n",
       "\n",
       "                                            keywords  similarity  \\\n",
       "3  [java, python, bachelor, master, sql, spark, s...    0.222222   \n",
       "1             [python, bachelor, master, sql, spark]    0.125000   \n",
       "0                                 [bachelor, master]    0.000000   \n",
       "2                                 [bachelor, master]    0.000000   \n",
       "4                  [undergraduate, master, mba, phd]    0.000000   \n",
       "\n",
       "   Minhash_Similarity  \n",
       "3            0.393494  \n",
       "1            0.344456  \n",
       "0            0.359651  \n",
       "2            0.359651  \n",
       "4            0.435641  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "# If city included, only search and recommend jobs in the city\n",
    "location = 'Boston,MA'\n",
    "#Scrape from web or read from local saved -----\n",
    "jobs_info = get_jobs_info(location)    \n",
    "# Keyword extraction and analysis \n",
    "# Initialize skill_keyword_match with job_info\n",
    "skill_match = skill_keyword_match(jobs_info)\n",
    "# Extract skill keywords from job descriptions \n",
    "skill_match.extract_jobs_keywords()\n",
    "#Job recommendation based on skill matching -----\n",
    "resume_skills = skill_match.extract_resume_keywords(SAMPLE_RESUME_PDF)\n",
    "print(\"resume skills : \", resume_skills)\n",
    "# Calculate similarity of skills from a resume and job posts \n",
    "top_job_matches = skill_match.cal_similarity(resume_skills.index, location)\n",
    "print(\"Top matches with jaccard similarity\\n:\")\n",
    "print(top_job_matches)\n",
    "top_job_matches_minhash = skill_match.calculate_minhash(resume_skills.index)\n",
    "print(\"\\nTop matches with cosine similarity using minhash\\n:\")\n",
    "print(top_job_matches_minhash)\n",
    "top_job_matches_minhash\n",
    "# Save matched jobs to a file\n",
    "#top_job_matches.to_csv(RECOMMENDED_JOBS_FILE+location+'.csv', index=False)\n",
    "#print('File of recommended jobs saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "uyYlTBQWKbsl",
    "outputId": "8ca7f6b8-c9f8-41b8-994c-4f1766027964",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "def main():\n",
    "    # If city included, only search and recommend jobs in the city\n",
    "    location = 'Boston,MA'\n",
    "    #Scrape from web or read from local saved -----\n",
    "    jobs_info = get_jobs_info(location)    \n",
    "    # print(\"Before:\\n\")\n",
    "    # print(jobs_info)\n",
    "    # Keyword extraction and analysis \n",
    "    # Initialize skill_keyword_match with job_info\n",
    "    skill_match = skill_keyword_match(jobs_info)\n",
    "    # print(\"\\nAfter:\\n\")\n",
    "    # print(jobs_info)\n",
    "    # Extract skill keywords from job descriptions \n",
    "    # print(dir(skill_match))\n",
    "    skill_match.extract_jobs_keywords()\n",
    "        \n",
    "    #Job recommendation based on skill matching -----\n",
    "    resume_skills = skill_match.extract_resume_keywords(SAMPLE_RESUME_PDF)\n",
    "    print(\"resume skills : \", resume_skills)\n",
    "    # Calculate similarity of skills from a resume and job posts \n",
    "    top_job_matches = skill_match.cal_similarity(resume_skills.index, location)\n",
    "    print(\"Top matches with jaccard similarity\\n:\")\n",
    "    print(top_job_matches)\n",
    "    top_job_matches_minhash = skill_match.calculate_minhash(resume_skills.index)\n",
    "    print(\"\\nTop matches with cosine similarity using minhash\\n:\")\n",
    "    print(top_job_matches_minhash)\n",
    "\n",
    "    # Save matched jobs to a file\n",
    "    #top_job_matches.to_csv(RECOMMENDED_JOBS_FILE+location+'.csv', index=False)\n",
    "    #print('File of recommended jobs saved')\n",
    "    \n",
    "     \n",
    "if __name__ == \"__main__\": \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
